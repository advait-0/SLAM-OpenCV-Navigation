# Neural Networks

## What are neurons?
Consider them to be packets **containing activation numbers.**

## What are activation numbers?
They are values between 0 and 1 which give the intensity of the pixel ie. 0 for black while 1 for white.

![Neuron](neuron.png)


### For a 28x28 pixel image, we have 784 neurons in our first layer.
<p></p>

### The second and third layer can contain any number of neurons. 
<p></p>

### Activations in the first layer leads to activation in the middle two layers. The final activation in the output layer gives one digit as output. This is the input digit to be recognized. 


### Why are layers used?
#### The layers help in dividing the input number into parts which can be recognized as loops, straight lines, etc.
<p> A neuron in any layer (from the second) corresponds to one of the given shapes.</p>

## How to recognize the sub-components (loops)?
<p>We recognize the different edges which make up the sub-component for eg. in a loop</p>

![Loop](loop.png)

## Taking an eg of number 9
<p>1. The first layer actiavtes only those neurons of the second layer whixh identify the sub-component loop of the number 9.</p>
<p>2. In the third layer, only those neurons are activated which identify the loop and the line of the number 9.</p>
<p>3. In the final layer, the neuron representing number 9 is activated and hence it is the recogized digit.</p>

## How does a neuron detect an edge?
<p>1. Consider a neuron in the second layerhas to conclude whether or not there exits an edge in a particular region.</p>
<p>2. We assign weights to the connection of our neuron in the second layer to each neuron in the first layer.</p>

![layers](1.png)

## Weighted sum
<p>We calculate the weighted sum and to get an output within the range of 0 to 1, we put it as input to the sigmoid function.</p>

![sigmoid function](sigmoid.png)

## Bias
<p>Before using the sigmoid function, we have a bias to get the weighted sum within a certain range.</p>

## Each neuron in the first layer is connected to all the neurons of the second layer. Each has a different weight and a different bias.

![matrix](wnc.png)

