# Neural Networks

## What are neurons?
Consider them to be packets **containing activation numbers.**

## What are activation numbers?
They are values between 0 and 1 which give the intensity of the pixel ie. 0 for black while 1 for white.

![Neuron](neuron.png)


### For a 28x28 pixel image, we have 784 neurons in our first layer.
<p></p>

### The second and third layer can contain any number of neurons. 
<p></p>

### Activations in the first layer leads to activation in the middle two layers. The final activation in the output layer gives one digit as output. This is the input digit to be recognized. 


### Why are layers used?
#### The layers help in dividing the input number into parts which can be recognized as loops, straight lines, etc.
<p> A neuron in any layer (from the second) corresponds to one of the given shapes.</p>

## How to recognize the sub-components (loops)?
<p>We recognize the different edges which make up the sub-component for eg. in a loop</p>

![Loop](loop.png)

## Taking an eg of number 9
<p>1. The first layer actiavtes only those neurons of the second layer whixh identify the sub-component loop of the number 9.</p>
<p>2. In the third layer, only those neurons are activated which identify the loop and the line of the number 9.</p>
<p>3. In the final layer, the neuron representing number 9 is activated and hence it is the recogized digit.</p>

## How does a neuron detect an edge?
<p>1. Consider a neuron in the second layerhas to conclude whether or not there exits an edge in a particular region.</p>
<p>2. We assign weights to the connection of our neuron in the second layer to each neuron in the first layer.</p>

![layers](1.png)

## Weighted sum
<p>We calculate the weighted sum and to get an output within the range of 0 to 1, we put it as input to the sigmoid function.</p>

![sigmoid function](sigmoid.png)

## Bias
<p>Before using the sigmoid function, we have a bias to get the weighted sum within a certain range.</p>

## Each neuron in the first layer is connected to all the neurons of the second layer. Each has a different weight and a different bias.

![matrix](wnc.png)

---

### How are the connections between the layers made?
<p>At first the connections are random. Their weights and biases are selected randomly. </p>

<p>The computer is told that it has made an error by creating a cost function.</p>

<p>1. Suppose, the input to the computer is the number 3, it should have 'activation 1' for the number 3 in its last layer and 'activation 0' for all the other numbers.</p>

<p>2. You add the squares of the differences of 1- the trash activation values (random) and 2-the activation values you want them to have (1 for desired number and 0 for others) </p>

<p>3. The result is the cost. </p>

## min cost: Good guessing of numbers
<p>4. The average of all the cost values (obtained from the various 'training' data) is taken. That average cost is the final output cost. </p>

<p>5. We will obtain the minimum cost when the derivative of the cost function is zero. </p>

<p>6. Instead of calculating the derivative of a complex function, we can move across its graph. When the slope's line is increasing, we shift to the left for lower value of cost. When it is decreasing, we shift to the right for lower value of cost.</p>

![How to find min of cost function](2.png)

<p>7. There is a gradient vector(multivariable calculus) which helps us find the local minimas of a function. </p>
<p>The algorithm is:</p>
<p>1. Find the direction os negative slope.</p>
<p>2. Take a small step in the direction of decreasing gradient.</p>
<p>3. Repeat the process.</p>

### This process of lowering the output cost is known as 'Back Propagation'.

## What does each component of input vector depict?
![Meaning of vector components](3..png)

## Example,
### Here the value of x affects the cost function 3 times more than the amount by which y affects it.

![function example](4.png)

## Limitations
<p>1. When given any random input (not a number), it stills gives out a number output.</p>
<p>2. It cannot draw a number given as input. </p>

---
## Backward Propagation

### Suppose the input number is 2

### <p>* For the activation of number 2 to be the highest in the last layer, we should get a high weighted sum for '2'. How is this done?</p>

<p>1. The connections (with the previous layer) linking high activation numbers should be given higher weights. ie. weights of connections should be in proportion with the respective activation.</p>
<p>2. The bias should be inceased for the number 2.</p>
<p>3. The activations of the previous layer should be increased for connections with higher weights.</p>

 ### <p>* At the same time, activation (in the last layer) for all the other numbers shoud be decreased. How is this done?</p>

 <p>Opposite to the above points.</p>

 ---

 <p>Each of the neurons for the remaining numbers will have their own thoughts on how to change the activation of the neurons of the previous layer. The effect of all the neurons is taken simultaneously and the activation of neurons in the previous layer is changed accordingly. </p>

 ![Effect on activation of neurons of previous layer](5.png)

 ## The average of the nudges desired by each number in the values of weights and biases is used to calculate the negative gradient of the cost function. 

 ## Stochastic gradient descent
 <p>It takes a lot of time for calculating the data using so many training examples. So the training data is grouped into 100 numbers and the data is calculated. </p>